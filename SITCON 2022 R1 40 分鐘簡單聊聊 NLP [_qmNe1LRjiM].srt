1
00:00:00,000 --> 00:00:10,000
大家好,我是這場議程的講者,我叫Andy Chen

2
00:00:10,000 --> 00:00:15,000
我今天要帶來的主題就是40分鐘簡單聊聊NLP

3
00:00:15,000 --> 00:00:20,000
首先先自我介紹一下,我叫Andy Chen,本名是江上軒

4
00:00:20,000 --> 00:00:24,000
我現在就讀國立中興大學資工系大三升大四

5
00:00:24,000 --> 00:00:28,000
同時也在中興大學的NLP實驗室擔任研究助理

6
00:00:28,000 --> 00:00:33,000
今年暑假有去工研院的資料服務與智慧決策部擔任實習生

7
00:00:33,000 --> 00:00:39,000
我主要的研究領域有網頁前後端、機器學習和自然語言處理

8
00:00:39,000 --> 00:00:44,000
在C-COM官網的自我介紹裡面就有提到了

9
00:00:44,000 --> 00:00:48,000
我自己已經參加過C-COM兩年了,所以今年我就開始想

10
00:00:48,000 --> 00:00:52,000
我已經當聽眾兩年了,今年還要繼續當下去嗎?

11
00:00:52,000 --> 00:00:54,000
還是就來當當看講者好了?

12
00:00:54,000 --> 00:00:57,000
所以我就跑去投稿,然後很幸運的就上了

13
00:00:57,000 --> 00:01:00,000
這就是為什麼我現在站在這裡

14
00:01:00,000 --> 00:01:04,000
在這裡也是想要鼓勵大家不要害怕嘗試

15
00:01:04,000 --> 00:01:08,000
說不定明年在台上的就是你了

16
00:01:08,000 --> 00:01:13,000
在開始之前,今天的投影片連結在左邊這裡

17
00:01:13,000 --> 00:01:18,000
大家可以掃QR Code,待會邊聽的時候可以邊參考

18
00:01:18,000 --> 00:01:21,000
另外右邊這裡是議程的資訊

19
00:01:21,000 --> 00:01:25,000
上面有議程的共筆

20
00:01:25,000 --> 00:01:29,000
大家可以在聽簡報的時候在上面做筆記

21
00:01:29,000 --> 00:01:31,000
也有slido的連結

22
00:01:31,000 --> 00:01:35,000
如果議程中有任何問題也可以上去提問

23
00:01:35,000 --> 00:01:39,000
最後會保留大約五分鐘來進行QA

24
00:01:39,000 --> 00:01:43,000
我會把畫面停在這個畫面

25
00:01:43,000 --> 00:01:46,000
一段時間讓大家可以掃QR Code

26
00:01:46,000 --> 00:01:49,000
也順便趁這個時候調查一下

27
00:01:49,000 --> 00:01:55,000
可以請有聽過NLP的聽眾舉一下手嗎

28
00:01:55,000 --> 00:01:57,000
好 可以放下了

29
00:01:57,000 --> 00:01:59,000
大概一半左右

30
00:01:59,000 --> 00:02:01,000
沒有聽過的也沒有關係

31
00:02:01,000 --> 00:02:05,000
因為待會基本上就是講NLP的科普

32
00:02:05,000 --> 00:02:09,000
就把它當作聽聽故事就可以了

33
00:02:09,000 --> 00:02:13,000
那我們就開始今天的議程

34
00:02:13,000 --> 00:02:15,000
首先目錄我分成了六個章節

35
00:02:15,000 --> 00:02:19,000
分別就是用what why when跟how來做區分

36
00:02:19,000 --> 00:02:22,000
最後是reference跟summary

37
00:02:22,000 --> 00:02:26,000
這場議程主要是想讓大家對NLP產生興趣

38
00:02:26,000 --> 00:02:29,000
並且知道有什麼工具可以使用

39
00:02:29,000 --> 00:02:32,000
還有看滿滿的梗圖

40
00:02:32,000 --> 00:02:36,000
首先就先來NLP的簡介

41
00:02:36,000 --> 00:02:38,000
你知道NLP是什麼嗎

42
00:02:38,000 --> 00:02:40,000
雖然我不是NLP專家

43
00:02:40,000 --> 00:02:42,000
但這聽起來很讚吧

44
00:02:42,000 --> 00:02:47,000
NLP全名就是Natural Language Processing

45
00:02:47,000 --> 00:02:49,000
簡稱就是NLP

46
00:02:49,000 --> 00:02:52,000
中文就叫自然語言處理

47
00:02:52,000 --> 00:02:54,000
也可以把它想像成是一種

48
00:02:54,000 --> 00:02:57,000
電腦科學加上語言學的一門學問

49
00:02:57,000 --> 00:03:01,000
它也是人工智慧底下的一門分支

50
00:03:01,000 --> 00:03:03,000
簡單用一句話形容

51
00:03:03,000 --> 00:03:06,000
NLP就是探討如何讓電腦理解

52
00:03:06,000 --> 00:03:09,000
甚至去運用我們的自然語言

53
00:03:09,000 --> 00:03:11,000
這邊就有一個詞需要解釋了

54
00:03:11,000 --> 00:03:14,000
就是自然語言

55
00:03:14,000 --> 00:03:16,000
自然語言就是

56
00:03:16,000 --> 00:03:19,000
人類為了溝通所創造出來的語言

57
00:03:19,000 --> 00:03:21,000
形式可以是文字

58
00:03:21,000 --> 00:03:22,000
也可以是語音

59
00:03:22,000 --> 00:03:24,000
甚至是符號

60
00:03:24,000 --> 00:03:28,000
這些通通都可以叫做自然語言

61
00:03:28,000 --> 00:03:30,000
所以這邊就會有分兩種

62
00:03:30,000 --> 00:03:32,000
一種是人類的語言

63
00:03:32,000 --> 00:03:34,000
就是給人來看的

64
00:03:34,000 --> 00:03:36,000
另外一個是數值的資料

65
00:03:36,000 --> 00:03:40,000
就是給機器看的

66
00:03:40,000 --> 00:03:42,000
NLP又可以細分成兩種

67
00:03:42,000 --> 00:03:44,000
一種是你把人類的語言

68
00:03:44,000 --> 00:03:45,000
轉成數值的資料

69
00:03:45,000 --> 00:03:48,000
這種我們叫做自然語言理解

70
00:03:48,000 --> 00:03:50,000
有簡稱NLU

71
00:03:50,000 --> 00:03:51,000
換句話說就是讓機器

72
00:03:51,000 --> 00:03:54,000
可以讀懂人類的語言

73
00:03:54,000 --> 00:03:56,000
反過來將數值資料

74
00:03:56,000 --> 00:03:57,000
轉成人類的語言

75
00:03:57,000 --> 00:03:59,000
這個我們就叫自然語言生成

76
00:03:59,000 --> 00:04:01,000
有簡稱NLG

77
00:04:01,000 --> 00:04:02,000
換句話說就是讓機器

78
00:04:02,000 --> 00:04:05,000
可以創造出人類的語言

79
00:04:05,000 --> 00:04:09,000
自然語言理解就細分成這兩種

80
00:04:09,000 --> 00:04:12,000
接下來介紹NLP的實際應用

81
00:04:12,000 --> 00:04:15,000
首先第一個是email的篩選器

82
00:04:15,000 --> 00:04:17,000
像是可以根據你的信件內容

83
00:04:17,000 --> 00:04:19,000
來過濾出垃圾郵件

84
00:04:19,000 --> 00:04:20,000
或者像Gmail

85
00:04:20,000 --> 00:04:22,000
會幫你把信件分成主要

86
00:04:22,000 --> 00:04:24,000
然後社交跟促銷

87
00:04:24,000 --> 00:04:28,000
讓你的收件夾看起來比較整齊

88
00:04:28,000 --> 00:04:30,000
還有像是情感分析

89
00:04:30,000 --> 00:04:33,000
就是公司可以去社群媒體上面

90
00:04:33,000 --> 00:04:35,000
蒐集他們客戶對於自家產品的

91
00:04:35,000 --> 00:04:37,000
一些留言或是貼文

92
00:04:37,000 --> 00:04:39,000
然後分析他是正面還是負面的

93
00:04:39,000 --> 00:04:43,000
然後來即時改善他們的行銷手法

94
00:04:43,000 --> 00:04:45,000
再來智能助理

95
00:04:45,000 --> 00:04:48,000
比較常聽到的像是Google Assistant

96
00:04:48,000 --> 00:04:49,000
Apple的Siri

97
00:04:49,000 --> 00:04:52,000
或是Amazon的Alexa這些

98
00:04:52,000 --> 00:04:54,000
這些已經漸漸成為

99
00:04:54,000 --> 00:04:56,000
我們日常生活的一部分

100
00:04:56,000 --> 00:04:57,000
除了可以幫我們處理

101
00:04:57,000 --> 00:04:59,000
各種生活瑣事之外

102
00:04:59,000 --> 00:05:01,000
無聊的時候也可以跟他聊天

103
00:05:01,000 --> 00:05:03,000
或講笑話之類的

104
00:05:03,000 --> 00:05:06,000
因為我們NLP實際的案例實在太多了

105
00:05:06,000 --> 00:05:08,000
所以我們這邊就只舉這三個

106
00:05:08,000 --> 00:05:11,000
來當作例子

107
00:05:11,000 --> 00:05:14,000
接下來來介紹NLP的發展史

108
00:05:14,000 --> 00:05:16,000
首先在1950年代

109
00:05:16,000 --> 00:05:18,000
那個時候還沒有機器學習的概念

110
00:05:18,000 --> 00:05:21,000
所以當時候只能透過語言學的角度

111
00:05:21,000 --> 00:05:23,000
來分析出語言的規則

112
00:05:23,000 --> 00:05:25,000
再把它寫成電腦程式

113
00:05:25,000 --> 00:05:27,000
但是這種方法就一定很差

114
00:05:27,000 --> 00:05:29,000
因為語言有太多例外了

115
00:05:29,000 --> 00:05:32,000
相信學過高中英文的人就知道

116
00:05:32,000 --> 00:05:36,000
就是英文的動詞不規則三態變化

117
00:05:36,000 --> 00:05:38,000
就讓人非常頭痛

118
00:05:38,000 --> 00:05:41,000
所以有時候連人自己都搞不清楚了

119
00:05:41,000 --> 00:05:44,000
那更何況是給電腦呢

120
00:05:44,000 --> 00:05:47,000
所以因此是一直到1980年代

121
00:05:47,000 --> 00:05:48,000
NLP才開始

122
00:05:48,000 --> 00:05:50,000
就是帶入了機器學習的概念之後

123
00:05:50,000 --> 00:05:54,000
那才開始有比較蓬勃的發展

124
00:05:54,000 --> 00:05:57,000
那這個一開始的做法

125
00:05:57,000 --> 00:05:59,000
就是會收集很多的文本

126
00:05:59,000 --> 00:06:01,000
那我們會把它稱作語料庫

127
00:06:01,000 --> 00:06:04,000
那接著會用這些語料庫來訓練模型

128
00:06:04,000 --> 00:06:08,000
然後來讓它找出這些單字之間的關聯

129
00:06:08,000 --> 00:06:13,000
那這種做法比起你用人工規則就更有彈性

130
00:06:13,000 --> 00:06:16,000
那也能處理比較超出範圍的問題

131
00:06:16,000 --> 00:06:21,000
那接下來就介紹幾個比較有名的機器學習的模型

132
00:06:21,000 --> 00:06:24,000
首先第一個要介紹的是Word2Vector

133
00:06:24,000 --> 00:06:27,000
那Word2Vector輸入是一個單字

134
00:06:27,000 --> 00:06:29,000
輸出就是代表這個單字的向量

135
00:06:29,000 --> 00:06:35,000
所以可以看到每一個單字都會對應到一個唯一的向量

136
00:06:35,000 --> 00:06:40,000
那訓練過程簡單來講就是透過它的上下文

137
00:06:40,000 --> 00:06:43,000
來學習到單字之間的關係

138
00:06:43,000 --> 00:06:44,000
比方說這邊有一句

139
00:06:44,000 --> 00:06:46,000
我明天要搭火車去台北

140
00:06:46,000 --> 00:06:47,000
跟另外一句

141
00:06:47,000 --> 00:06:49,000
我明天要搭飛機去台北

142
00:06:49,000 --> 00:06:53,000
那這裡因為火車跟飛機它的前後文都一樣

143
00:06:53,000 --> 00:06:56,000
所以當模型看過很多這樣的句子之後

144
00:06:56,000 --> 00:06:59,000
就會知道火車跟飛機是兩個很相近的詞

145
00:06:59,000 --> 00:07:02,000
然後就會在向量的空間上面

146
00:07:02,000 --> 00:07:04,000
給它們兩個很相似的向量

147
00:07:04,000 --> 00:07:05,000
就像這張圖

148
00:07:05,000 --> 00:07:07,000
那比較不相關的單字

149
00:07:07,000 --> 00:07:08,000
比方說貓咪

150
00:07:08,000 --> 00:07:13,000
就會得到一個差距比較遠的向量

151
00:07:13,000 --> 00:07:15,000
那Word2Veter簡單來講就是這樣

152
00:07:15,000 --> 00:07:17,000
那雖然聽起來很棒

153
00:07:17,000 --> 00:07:20,000
但是它還是存在著一些缺點

154
00:07:20,000 --> 00:07:23,000
首先是它沒有辦法處理一字多語的問題

155
00:07:23,000 --> 00:07:24,000
因為剛才有講到

156
00:07:24,000 --> 00:07:27,000
Word2Veter一個單字就是對應到一個唯一的向量

157
00:07:27,000 --> 00:07:30,000
所以當你一個詞有兩個意思的時候

158
00:07:30,000 --> 00:07:33,000
這個Word2Veter是沒有辦法解決的

159
00:07:33,000 --> 00:07:34,000
那還有就是

160
00:07:34,000 --> 00:07:36,000
它不考慮到詞的先後順序

161
00:07:36,000 --> 00:07:38,000
比方說下面這個例子

162
00:07:38,000 --> 00:07:40,000
我借小明一百元

163
00:07:40,000 --> 00:07:42,000
跟小明借我一百元

164
00:07:42,000 --> 00:07:44,000
這兩個句子很明顯是不同的

165
00:07:44,000 --> 00:07:46,000
但是對Word2Veter來說

166
00:07:46,000 --> 00:07:48,000
因為它的前後文都相同

167
00:07:48,000 --> 00:07:49,000
所以對Word2Veter來說

168
00:07:49,000 --> 00:07:52,000
這兩個句子是相同的

169
00:07:52,000 --> 00:07:55,000
那這個問題就要用RN來解決

170
00:07:55,000 --> 00:07:58,000
那RN的話就像我們閱讀的習慣一樣

171
00:07:58,000 --> 00:07:59,000
是從左到右

172
00:07:59,000 --> 00:08:02,000
一個一個字這樣子讀過來

173
00:08:02,000 --> 00:08:04,000
那這樣就可以根據你前文讀到的內容

174
00:08:04,000 --> 00:08:09,000
而對後文產生不一樣的結果

175
00:08:09,000 --> 00:08:11,000
那再來就到了2018年

176
00:08:11,000 --> 00:08:15,000
Google提出了BERT這個語言域訓練模型之後

177
00:08:15,000 --> 00:08:18,000
就開始對NLP帶來革命性的突破

178
00:08:18,000 --> 00:08:21,000
那為什麼要放下面這張圖呢

179
00:08:21,000 --> 00:08:24,000
其實黃色這個角色就叫做BERT

180
00:08:24,000 --> 00:08:27,000
他是芝麻街裡面的一個角色

181
00:08:27,000 --> 00:08:30,000
有趣的是之後很多NLP的模型

182
00:08:30,000 --> 00:08:34,000
都會以芝麻街角色的名字來命名

183
00:08:34,000 --> 00:08:38,000
像是有什麼Ernie或是Elmo這些

184
00:08:38,000 --> 00:08:41,000
有些很明顯甚至是硬湊出來的

185
00:08:41,000 --> 00:08:45,000
就很硬要不知道在說什麼

186
00:08:45,000 --> 00:08:47,000
那在介紹BERT之前呢

187
00:08:47,000 --> 00:08:49,000
還要先知道這個Transformer

188
00:08:49,000 --> 00:08:54,000
那Transformer是近年來相當熱門的一個深度學習的模型

189
00:08:54,000 --> 00:08:56,000
那它是使用到那個

190
00:08:56,000 --> 00:08:59,000
Self-Attention這一個機制

191
00:08:59,000 --> 00:09:05,000
那它的架構實際上分成Encoder跟Decoder兩個部分

192
00:09:05,000 --> 00:09:08,000
而BERT呢其實就是它Encoder的部分

193
00:09:08,000 --> 00:09:12,000
也就是紅色框框圈起來的這一塊

194
00:09:12,000 --> 00:09:14,000
那關於Transformer的話

195
00:09:14,000 --> 00:09:17,000
這邊因為時間不夠所以不會再多說

196
00:09:17,000 --> 00:09:19,000
想要了解的話就回家Google一下

197
00:09:19,000 --> 00:09:22,000
就會有很多資源了

198
00:09:22,000 --> 00:09:26,000
那我們來講一下Google怎麼訓練BERT這樣一個模型

199
00:09:26,000 --> 00:09:29,000
首先他們收集了很大量的語料庫

200
00:09:29,000 --> 00:09:33,000
這邊是用Book Opus加上English的Wikipedia

201
00:09:33,000 --> 00:09:35,000
總共有33億個字

202
00:09:35,000 --> 00:09:39,000
然後透過非監督式的方式來做Pre-Train

203
00:09:39,000 --> 00:09:42,000
然後就拿這個Pre-Train好的模型

204
00:09:42,000 --> 00:09:45,000
去針對特定的下游任務來做Fighting

205
00:09:45,000 --> 00:09:49,000
結果就是都比傳統的模型來得好

206
00:09:49,000 --> 00:09:53,000
所以當年是橫掃了整個NLP任務的排行榜

207
00:09:53,000 --> 00:09:56,000
就是所向無敵這樣

208
00:09:56,000 --> 00:09:59,000
那這個概念一開始聽到的時候可能會無法理解

209
00:09:59,000 --> 00:10:00,000
為什麼會這樣

210
00:10:00,000 --> 00:10:04,000
那這個概念其實就很像我們學中文一樣

211
00:10:04,000 --> 00:10:07,000
因為我們從小到大都在接觸中文

212
00:10:07,000 --> 00:10:11,000
所以我們耳濡目染之下對中文就有基本的預感

213
00:10:11,000 --> 00:10:14,000
所以這個時候如果再叫我們去學一些特定的任務

214
00:10:14,000 --> 00:10:17,000
比方說接龍或照樣造句這類的

215
00:10:17,000 --> 00:10:21,000
那我們比起可能從來沒有碰過中文的外國人來說

216
00:10:21,000 --> 00:10:24,000
我們學起來就會輕鬆很多

217
00:10:24,000 --> 00:10:26,000
所以我們在做pre-train的時候

218
00:10:26,000 --> 00:10:29,000
其實就是讓模型先學會一個基本的預感

219
00:10:29,000 --> 00:10:31,000
然後對他做fine-tune的時候

220
00:10:31,000 --> 00:10:35,000
就是讓他去學習一些特定的任務

221
00:10:35,000 --> 00:10:37,000
那有了這個概念之後

222
00:10:37,000 --> 00:10:39,000
但除非你是那種很大的企業

223
00:10:39,000 --> 00:10:42,000
不然你想要自己pre-train一個語言訓練模型

224
00:10:42,000 --> 00:10:44,000
根本是天方夜譚

225
00:10:44,000 --> 00:10:46,000
那這邊就有一個例子

226
00:10:46,000 --> 00:10:50,000
比方說你想要訓練一個1.1億參數的BERT-based模型的話

227
00:10:50,000 --> 00:10:53,000
那你要花16個TPU,然後要跑4天

228
00:10:53,000 --> 00:10:58,000
而且你還要先收集到33億個字的預料庫才行

229
00:10:58,000 --> 00:11:01,000
所以這對一般的人來說是非常困難的

230
00:11:01,000 --> 00:11:06,000
那幸好BERT的作者就有開源他們pre-train好的模型

231
00:11:06,000 --> 00:11:09,000
那我們就可以直接站在巨人的肩膀上

232
00:11:09,000 --> 00:11:12,000
執行下的任務變得既輕鬆又有效

233
00:11:12,000 --> 00:11:15,000
那這邊也稍微補充一下

234
00:11:15,000 --> 00:11:18,000
就是你把這種pre-train好的模型開源出來

235
00:11:18,000 --> 00:11:20,000
其實也是有助於環境的

236
00:11:20,000 --> 00:11:23,000
因為就有研究指出你訓練一個BERT

237
00:11:23,000 --> 00:11:25,000
那因為它會耗電嘛

238
00:11:25,000 --> 00:11:28,000
所以它產生出來的碳足跡是0.63噸

239
00:11:28,000 --> 00:11:35,000
就相當於你的飛機從紐約飛往舊金山來回一趟的碳排量

240
00:11:35,000 --> 00:11:38,000
所以當你把這個pre-train好的模型開源出來

241
00:11:38,000 --> 00:11:40,000
那大家就不用重複的一直pre-train

242
00:11:40,000 --> 00:11:44,000
那就可以減少這個碳的排量

243
00:11:44,000 --> 00:11:50,000
那也因此就近年來NLP的研究現況就像這張圖

244
00:11:50,000 --> 00:11:52,000
就是NLP的研究者呢

245
00:11:52,000 --> 00:11:54,000
大家都跑去研究BERT了

246
00:11:54,000 --> 00:11:58,000
那新的那些論文也幾乎都是以BERT為主

247
00:11:58,000 --> 00:12:00,000
用傳統模型的人就少很多

248
00:12:00,000 --> 00:12:05,000
那就可見NLP的有這麼強大

249
00:12:05,000 --> 00:12:08,000
那自從Google提出了BERT之後

250
00:12:08,000 --> 00:12:11,000
那其他的那些大企業跟組織也就跟著推出了

251
00:12:11,000 --> 00:12:13,000
他們的語言域訓練模型

252
00:12:13,000 --> 00:12:16,000
那我們就直接看這張圖

253
00:12:16,000 --> 00:12:17,000
首先BERT在這裡

254
00:12:17,000 --> 00:12:20,000
它是340M的參數

255
00:12:20,000 --> 00:12:23,000
然後這邊是OpenAI的GPT-2

256
00:12:23,000 --> 00:12:26,000
它是1500M的參數

257
00:12:26,000 --> 00:12:28,000
然後還有Facebook的Robota

258
00:12:28,000 --> 00:12:31,000
跟Google後來又有一個T5模型

259
00:12:31,000 --> 00:12:34,000
那這個是Microsoft的Tooling NLG

260
00:12:34,000 --> 00:12:37,000
那個時候就有17000M的參數

261
00:12:37,000 --> 00:12:39,000
總之後面的參數

262
00:12:39,000 --> 00:12:40,000
就是一個比一個還要大

263
00:12:40,000 --> 00:12:42,000
就越來越誇張

264
00:12:42,000 --> 00:12:45,000
那剛才那個只有到2020年

265
00:12:45,000 --> 00:12:48,000
那這邊有到2022年的

266
00:12:48,000 --> 00:12:50,000
那所以後來又有一個GPT-3

267
00:12:50,000 --> 00:12:53,000
那它是175B的參數

268
00:12:53,000 --> 00:12:54,000
那後面這個更扯

269
00:12:54,000 --> 00:12:56,000
這個Mecatron Tooling NLG

270
00:12:56,000 --> 00:12:59,000
它有570B的參數

271
00:12:59,000 --> 00:13:02,000
反正就是超級無敵大

272
00:13:02,000 --> 00:13:05,000
那接下來就進入實作的部分

273
00:13:05,000 --> 00:13:07,000
那我們會用到

274
00:13:07,000 --> 00:13:09,000
Google Colab跟HackinFace

275
00:13:09,000 --> 00:13:11,000
那首先介紹一下Google Colab

276
00:13:11,000 --> 00:13:13,000
那相信只要有使用過

277
00:13:13,000 --> 00:13:14,000
JPTENotebook的人

278
00:13:14,000 --> 00:13:16,000
對這個一定就不陌生

279
00:13:16,000 --> 00:13:18,000
因為他們兩個就非常的像

280
00:13:18,000 --> 00:13:21,000
那首先列出它的一些優點

281
00:13:21,000 --> 00:13:23,000
第一個是你不需要額外架設環境

282
00:13:23,000 --> 00:13:25,000
你只要有網路 瀏覽器

283
00:13:25,000 --> 00:13:27,000
然後連上它的伺服器之後

284
00:13:27,000 --> 00:13:30,000
就可以執行Python的程式了

285
00:13:30,000 --> 00:13:31,000
那再來就是它內建

286
00:13:31,000 --> 00:13:33,000
有很多機器學習的套件

287
00:13:33,000 --> 00:13:37,000
像是這個PyTorch、TensorFlow這些

288
00:13:37,000 --> 00:13:38,000
那再來是

289
00:13:38,000 --> 00:13:42,000
它可以免費的讓你使用GPU跟TPU

290
00:13:42,000 --> 00:13:44,000
還有就是它存放在

291
00:13:44,000 --> 00:13:45,000
你的Google Drive上面

292
00:13:45,000 --> 00:13:47,000
所以不管是要分享還是共用

293
00:13:47,000 --> 00:13:50,000
都很容易

294
00:13:50,000 --> 00:13:52,000
那再來就是它可以視覺化的

295
00:13:52,000 --> 00:13:54,000
幫你 就是像一些圖片表格

296
00:13:54,000 --> 00:13:58,000
它可以視覺化的幫你呈現出來

297
00:13:58,000 --> 00:14:01,000
那再講一下它的缺點

298
00:14:01,000 --> 00:14:03,000
那它連續運行的最長時間

299
00:14:03,000 --> 00:14:04,000
就是12個小時

300
00:14:04,000 --> 00:14:05,000
那如果你超過的話

301
00:14:05,000 --> 00:14:07,000
就會被強制停止

302
00:14:07,000 --> 00:14:08,000
而且你下次重開的時候

303
00:14:08,000 --> 00:14:10,000
資料就不見了

304
00:14:10,000 --> 00:14:12,000
那再來就是它的GPU跟TPU

305
00:14:12,000 --> 00:14:14,000
是有用量限制的

306
00:14:14,000 --> 00:14:15,000
所以你只要用超過

307
00:14:15,000 --> 00:14:16,000
就會被ban掉

308
00:14:16,000 --> 00:14:18,000
那就除非你課金買Codec Pro

309
00:14:18,000 --> 00:14:21,000
那它的用量才會比較多

310
00:14:21,000 --> 00:14:23,000
那雖然有上面這些缺點

311
00:14:23,000 --> 00:14:25,000
但整體來說還是利大於弊

312
00:14:25,000 --> 00:14:26,000
所以還是很推薦

313
00:14:26,000 --> 00:14:29,000
給機器學習的初學者來使用

314
00:14:29,000 --> 00:14:31,000
那像我自己做專題的那個模型

315
00:14:31,000 --> 00:14:33,000
就通通都在Code Lab上面跑

316
00:14:33,000 --> 00:14:35,000
所以就完全沒花到半毛錢

317
00:14:35,000 --> 00:14:40,000
那過程中也是把GPU用爆好幾次

318
00:14:40,000 --> 00:14:43,000
那接下來介紹這個Hogging Face

319
00:14:43,000 --> 00:14:46,000
那它是一間人工智慧的新創公司

320
00:14:46,000 --> 00:14:48,000
那他們公司的宗旨就是

321
00:14:48,000 --> 00:14:50,000
要讓最先進的NLP模型

322
00:14:50,000 --> 00:14:52,000
可以易於使用

323
00:14:52,000 --> 00:14:54,000
所以他們就有開源很多

324
00:14:54,000 --> 00:14:56,000
很知名的語言域訓練模型

325
00:14:56,000 --> 00:15:00,000
像是剛才講的BERT GPT-2這些

326
00:15:00,000 --> 00:15:02,000
然後還有個人覺得他們網站做得蠻好看

327
00:15:02,000 --> 00:15:04,000
大家有空可以去看看

328
00:15:04,000 --> 00:15:06,000
那這個是他們公司的Logo

329
00:15:06,000 --> 00:15:09,000
就是一個擁抱的表情符號

330
00:15:09,000 --> 00:15:12,000
還蠻可愛的

331
00:15:12,000 --> 00:15:15,000
那再來講他們有一個Transformers套件

332
00:15:15,000 --> 00:15:17,000
那使用者可以用這個套件

333
00:15:17,000 --> 00:15:21,000
來輕易地下載訓練跟上傳語言域訓練模型

334
00:15:21,000 --> 00:15:25,000
那他也支援一百多種語言的文本分類

335
00:15:25,000 --> 00:15:27,000
生成和問答等任務

336
00:15:27,000 --> 00:15:29,000
那這個套件目前在GitHub上面

337
00:15:29,000 --> 00:15:31,000
有6.8萬個Star

338
00:15:31,000 --> 00:15:33,000
那這個Star的成長速度

339
00:15:33,000 --> 00:15:36,000
在新創公司裡面算是最快的

340
00:15:36,000 --> 00:15:39,000
那6.8萬雖然聽起來沒有很多

341
00:15:39,000 --> 00:15:40,000
但是我去找了一下

342
00:15:40,000 --> 00:15:42,000
就是PyTorch也才5.8萬而已

343
00:15:42,000 --> 00:15:44,000
那Psychic Learn也才5.1萬

344
00:15:44,000 --> 00:15:49,000
所以這個6.8萬真的是算很多了

345
00:15:49,000 --> 00:15:51,000
那再來的話是

346
00:15:51,000 --> 00:15:53,000
我自己有寫一個CodeLab notebook

347
00:15:53,000 --> 00:15:54,000
本來是想要直接demo

348
00:15:54,000 --> 00:15:57,000
但是因為聽說這邊網路可能不太給力

349
00:15:57,000 --> 00:15:59,000
所以我後來就放到投影片上講

350
00:15:59,000 --> 00:16:02,000
那這個CodeLab連結有放在這邊

351
00:16:02,000 --> 00:16:05,000
那大家回家之後可以自己去玩玩看

352
00:16:05,000 --> 00:16:07,000
那就是提醒一下大家

353
00:16:07,000 --> 00:16:10,000
在執行之前盡量先另存一個自己的副本

354
00:16:10,000 --> 00:16:13,000
然後還有就是執行階段要改成GPU

355
00:16:13,000 --> 00:16:15,000
不然你會訓練得非常慢

356
00:16:15,000 --> 00:16:18,000
那再來就是開始執行之前要點連線這樣

357
00:16:19,000 --> 00:16:21,000
那我們就來講這個

358
00:16:21,000 --> 00:16:24,000
實際上要怎麼用Transformer這個套件

359
00:16:24,000 --> 00:16:26,000
那因為CodeLab

360
00:16:26,000 --> 00:16:29,000
它內建是沒有Transformer跟Dataset

361
00:16:29,000 --> 00:16:30,000
這兩個套件的

362
00:16:30,000 --> 00:16:33,000
所以我們要先用Pipe來下載

363
00:16:33,000 --> 00:16:35,000
那基本上用法就跟你在

364
00:16:35,000 --> 00:16:38,000
自己電腦裡面下載Pipe套件都一樣

365
00:16:40,000 --> 00:16:42,000
那接下來我們就要知道

366
00:16:42,000 --> 00:16:45,000
這個NLP可以做什麼樣的任務

367
00:16:45,000 --> 00:16:47,000
那我們首先要用的是Pipeline這個功能

368
00:16:48,000 --> 00:16:50,000
那它也是你執行NLP任務

369
00:16:50,000 --> 00:16:53,000
一個最簡單的方式

370
00:16:53,000 --> 00:16:56,000
那首先我們要來做情感分析的任務

371
00:16:56,000 --> 00:16:58,000
所以我們就把PipelinePO進來

372
00:16:58,000 --> 00:17:02,000
然後這個參數就用Sentiment Analysis

373
00:17:02,000 --> 00:17:03,000
這樣就可以了

374
00:17:03,000 --> 00:17:05,000
那你甚至不用跟它講說

375
00:17:05,000 --> 00:17:06,000
你要用什麼模型

376
00:17:06,000 --> 00:17:09,000
因為它預設就會去找最好的模型

377
00:17:09,000 --> 00:17:11,000
然後把它下載下來

378
00:17:11,000 --> 00:17:15,000
那接下來就可以隨便給模型一句話

379
00:17:15,000 --> 00:17:16,000
比方說跟它講

380
00:17:16,000 --> 00:17:18,000
This movie is interesting

381
00:17:18,000 --> 00:17:20,000
那它的分析結果就是positive

382
00:17:20,000 --> 00:17:21,000
就是正面的

383
00:17:21,000 --> 00:17:23,000
那如果This movie is boring的話呢

384
00:17:23,000 --> 00:17:24,000
結果就是negative

385
00:17:24,000 --> 00:17:25,000
就是負面的

386
00:17:25,000 --> 00:17:27,000
所以它這樣子就可以做一個

387
00:17:27,000 --> 00:17:29,000
很簡單的情感分析

388
00:17:29,000 --> 00:17:31,000
那我們剛才用Pipeline就很簡單

389
00:17:31,000 --> 00:17:34,000
只用三行程式就解決了

390
00:17:34,000 --> 00:17:35,000
那你甚至不需要了解

391
00:17:35,000 --> 00:17:38,000
怎麼做資料預處理跟模型架構

392
00:17:38,000 --> 00:17:42,000
那因為這些Pipeline都已經幫你做好了

393
00:17:42,000 --> 00:17:47,000
那接下來試試看文本生成的任務

394
00:17:47,000 --> 00:17:50,000
那參數就用text generation

395
00:17:50,000 --> 00:17:53,000
那它預設會用GPT-2

396
00:17:53,000 --> 00:17:56,000
那接下來就給它一句話

397
00:17:56,000 --> 00:17:59,000
Once upon a time there were three little pigs

398
00:17:59,000 --> 00:18:00,000
這個是三隻小豬

399
00:18:00,000 --> 00:18:03,000
那我們看它會生什麼東西出來

400
00:18:03,000 --> 00:18:05,000
它就繼續接下去說

401
00:18:05,000 --> 00:18:07,000
She was pregnant with two

402
00:18:07,000 --> 00:18:09,000
One her father and one his mother

403
00:18:09,000 --> 00:18:11,000
And the baby boy blah blah blah

404
00:18:11,000 --> 00:18:13,000
不知道在說什麼

405
00:18:13,000 --> 00:18:18,000
但這個看起來句子還滿流暢的

406
00:18:18,000 --> 00:18:22,000
那我們再來試試看這個問答的任務

407
00:18:22,000 --> 00:18:25,000
那參數就是給question answer

408
00:18:25,000 --> 00:18:29,000
那它預設一樣會使用這個模型

409
00:18:29,000 --> 00:18:30,000
那這邊的問答呢

410
00:18:30,000 --> 00:18:32,000
是你給它一段文章

411
00:18:32,000 --> 00:18:33,000
然後再問它一個問題

412
00:18:33,000 --> 00:18:36,000
它會從文章裡面去找答案

413
00:18:36,000 --> 00:18:37,000
所以像我們這裡

414
00:18:37,000 --> 00:18:39,000
我們文章就是給它一段

415
00:18:39,000 --> 00:18:41,000
有關哈利波特的介紹

416
00:18:41,000 --> 00:18:43,000
那問題就是問它

417
00:18:43,000 --> 00:18:45,000
Who is the author of Harry Potter

418
00:18:45,000 --> 00:18:46,000
那它的回答結果

419
00:18:46,000 --> 00:18:47,000
真的就是J.K. Rowling

420
00:18:47,000 --> 00:18:48,000
那你不信的話

421
00:18:48,000 --> 00:18:51,000
可以自己去跑跑看

422
00:18:51,000 --> 00:18:54,000
就這個模型還滿厲害的

423
00:18:54,000 --> 00:18:55,000
好那像我們剛才

424
00:18:55,000 --> 00:18:57,000
都是使用它預設的模型

425
00:18:57,000 --> 00:18:58,000
但其實你也可以換成

426
00:18:58,000 --> 00:19:00,000
用自己想要的模型

427
00:19:00,000 --> 00:19:01,000
像是我們剛才是做

428
00:19:01,000 --> 00:19:03,000
英文的情感分析

429
00:19:03,000 --> 00:19:04,000
那如果我們現在想要做

430
00:19:04,000 --> 00:19:06,000
中文的情感分析的話

431
00:19:06,000 --> 00:19:07,000
很簡單

432
00:19:07,000 --> 00:19:08,000
那就是到HuggingFace

433
00:19:08,000 --> 00:19:10,000
有一個Models這個網頁

434
00:19:10,000 --> 00:19:12,000
那你就去那邊

435
00:19:12,000 --> 00:19:14,000
大概會長這個樣子

436
00:19:14,000 --> 00:19:15,000
那左邊這邊呢

437
00:19:15,000 --> 00:19:17,000
是你的篩選標籤

438
00:19:17,000 --> 00:19:19,000
就是可以依據任務啊

439
00:19:19,000 --> 00:19:21,000
或資料集等等

440
00:19:21,000 --> 00:19:22,000
那右邊這邊

441
00:19:22,000 --> 00:19:23,000
就是篩選出來的

442
00:19:23,000 --> 00:19:25,000
這個模型的清單

443
00:19:25,000 --> 00:19:27,000
對就有很多模型

444
00:19:27,000 --> 00:19:29,000
列在這裡

445
00:19:29,000 --> 00:19:30,000
那因為我們要做的是

446
00:19:30,000 --> 00:19:31,000
中文的情感分析

447
00:19:31,000 --> 00:19:33,000
所以我們就一步一步來

448
00:19:33,000 --> 00:19:34,000
首先呢

449
00:19:34,000 --> 00:19:35,000
這個情感分析

450
00:19:35,000 --> 00:19:36,000
很明顯就是

451
00:19:36,000 --> 00:19:37,000
你給他一段句子

452
00:19:37,000 --> 00:19:38,000
那他就會回答

453
00:19:38,000 --> 00:19:40,000
他是正面還是負面的

454
00:19:40,000 --> 00:19:41,000
所以他很明顯

455
00:19:41,000 --> 00:19:43,000
是一個Test Classification

456
00:19:43,000 --> 00:19:45,000
文本分類的任務

457
00:19:45,000 --> 00:19:46,000
那語言的話

458
00:19:46,000 --> 00:19:48,000
當然就是選Chinese

459
00:19:48,000 --> 00:19:49,000
那這樣子

460
00:19:49,000 --> 00:19:51,000
他就會列出很多的模型

461
00:19:51,000 --> 00:19:53,000
那我最後是挑用

462
00:19:53,000 --> 00:19:55,000
我最後是挑這個模型

463
00:19:55,000 --> 00:19:56,000
那通常名字裡面

464
00:19:56,000 --> 00:19:57,000
有Sentiment的

465
00:19:57,000 --> 00:19:58,000
就代表他是一個

466
00:19:58,000 --> 00:20:00,000
做情感分析任務的模型

467
00:20:00,000 --> 00:20:04,000
但你不一定要挑跟我一樣的

468
00:20:04,000 --> 00:20:07,000
那要使用這個模型的話

469
00:20:07,000 --> 00:20:08,000
就會用到

470
00:20:08,000 --> 00:20:11,000
Automodel for Sequential Classification

471
00:20:11,000 --> 00:20:13,000
這一個物件

472
00:20:13,000 --> 00:20:14,000
那後面這裡

473
00:20:14,000 --> 00:20:17,000
就是在指定他的任務的類型

474
00:20:17,000 --> 00:20:18,000
那我們剛才有講

475
00:20:18,000 --> 00:20:19,000
任務是一個

476
00:20:19,000 --> 00:20:21,000
Test Classification的任務

477
00:20:21,000 --> 00:20:22,000
所以這裡就要選用

478
00:20:22,000 --> 00:20:23,000
Sequential Classification

479
00:20:23,000 --> 00:20:26,000
這兩個是一樣的

480
00:20:26,000 --> 00:20:28,000
然後如果你不知道

481
00:20:28,000 --> 00:20:29,000
你的模型到底是哪一種

482
00:20:29,000 --> 00:20:31,000
你就用Automodel就對了

483
00:20:31,000 --> 00:20:33,000
反正他會自己幫你決定

484
00:20:33,000 --> 00:20:34,000
然後另外要用的是

485
00:20:34,000 --> 00:20:36,000
這個Auto-Tokenizer

486
00:20:36,000 --> 00:20:38,000
那他的功用是

487
00:20:38,000 --> 00:20:39,000
把文字轉成數字

488
00:20:39,000 --> 00:20:41,000
那這個待會我會再講

489
00:20:41,000 --> 00:20:42,000
那要使用他的話

490
00:20:42,000 --> 00:20:43,000
很簡單就是

491
00:20:43,000 --> 00:20:45,000
把他們兩個input進來

492
00:20:45,000 --> 00:20:47,000
然後用fromPageChain

493
00:20:47,000 --> 00:20:48,000
然後參數呢

494
00:20:48,000 --> 00:20:50,000
就是給你剛才複製的

495
00:20:50,000 --> 00:20:51,000
那一串名稱

496
00:20:51,000 --> 00:20:52,000
模型的名稱

497
00:20:52,000 --> 00:20:53,000
這樣就可以了

498
00:20:53,000 --> 00:20:56,000
那他就會把他下載下來

499
00:20:56,000 --> 00:20:57,000
那這邊先講一下

500
00:20:57,000 --> 00:20:58,000
就是Tokenizer

501
00:20:58,000 --> 00:21:01,000
實際上的運作是怎麼樣

502
00:21:01,000 --> 00:21:02,000
因為我們知道

503
00:21:02,000 --> 00:21:04,000
模型他只吃數字

504
00:21:04,000 --> 00:21:06,000
他不可能直接吃文字

505
00:21:06,000 --> 00:21:07,000
所以我們必須幫他

506
00:21:07,000 --> 00:21:10,000
把文字轉成數字

507
00:21:10,000 --> 00:21:13,000
那他這個轉換的過程

508
00:21:13,000 --> 00:21:14,000
就是這個樣子

509
00:21:14,000 --> 00:21:16,000
首先我們有一個句子

510
00:21:16,000 --> 00:21:18,000
Today is a good day

511
00:21:18,000 --> 00:21:19,000
然後呢

512
00:21:19,000 --> 00:21:20,000
他就會先切成

513
00:21:20,000 --> 00:21:21,000
一個一個的單字

514
00:21:21,000 --> 00:21:23,000
然後再把這些單字

515
00:21:23,000 --> 00:21:25,000
轉成對應的數字

516
00:21:25,000 --> 00:21:26,000
那這些數字呢

517
00:21:26,000 --> 00:21:27,000
就叫做Token

518
00:21:27,000 --> 00:21:28,000
那模型實際上

519
00:21:28,000 --> 00:21:30,000
就是吃這些Token

520
00:21:30,000 --> 00:21:33,000
他的Input

521
00:21:33,000 --> 00:21:34,000
好那

522
00:21:34,000 --> 00:21:35,000
用法呢

523
00:21:35,000 --> 00:21:36,000
就跟剛才一樣

524
00:21:36,000 --> 00:21:37,000
用Pipeline

525
00:21:37,000 --> 00:21:38,000
那只是後面Model

526
00:21:38,000 --> 00:21:40,000
跟Tokenizer這裡

527
00:21:40,000 --> 00:21:41,000
就改成用

528
00:21:41,000 --> 00:21:42,000
我們剛才下載的

529
00:21:42,000 --> 00:21:44,000
Model跟Tokenizer

530
00:21:44,000 --> 00:21:45,000
就可以了

531
00:21:45,000 --> 00:21:46,000
那接下來就可以

532
00:21:46,000 --> 00:21:48,000
做中文的情感分析

533
00:21:48,000 --> 00:21:49,000
比方說

534
00:21:49,000 --> 00:21:50,000
這部電影真有趣

535
00:21:50,000 --> 00:21:52,000
那他的回答就是Positive

536
00:21:52,000 --> 00:21:53,000
那這部電影真無聊的

537
00:21:53,000 --> 00:21:54,000
回答就是Negative

538
00:21:54,000 --> 00:21:55,000
對所以這樣子就可以

539
00:21:55,000 --> 00:21:59,000
做中文的情感分析了

540
00:21:59,000 --> 00:22:00,000
那我們剛才都是拿

541
00:22:00,000 --> 00:22:02,000
別人Fightone好的模型

542
00:22:02,000 --> 00:22:03,000
但這樣做就很遜

543
00:22:03,000 --> 00:22:04,000
所以我們現在

544
00:22:04,000 --> 00:22:05,000
想要自己來Fightone

545
00:22:05,000 --> 00:22:09,000
一個模型

546
00:22:09,000 --> 00:22:10,000
好那在Fightone模型之前呢

547
00:22:10,000 --> 00:22:13,000
我們必須要先找到資料集

548
00:22:13,000 --> 00:22:14,000
那

549
00:22:14,000 --> 00:22:15,000
要找資料集也很簡單

550
00:22:15,000 --> 00:22:16,000
就是到

551
00:22:16,000 --> 00:22:17,000
Parkingface有一個Dataset

552
00:22:17,000 --> 00:22:19,000
這個網站

553
00:22:19,000 --> 00:22:20,000
那看起來大概

554
00:22:20,000 --> 00:22:21,000
就會長這個樣子

555
00:22:21,000 --> 00:22:22,000
跟這個Model

556
00:22:22,000 --> 00:22:24,000
基本上大同小異

557
00:22:24,000 --> 00:22:26,000
左邊一樣是你的篩選標籤

558
00:22:26,000 --> 00:22:27,000
那右邊就是

559
00:22:27,000 --> 00:22:31,000
資料集的清單

560
00:22:31,000 --> 00:22:32,000
好那因為我們現在要做的是

561
00:22:32,000 --> 00:22:34,000
英文的情感分析

562
00:22:34,000 --> 00:22:37,000
所以我們就一步一步來

563
00:22:37,000 --> 00:22:39,000
首先這個問題的分類

564
00:22:39,000 --> 00:22:41,000
就是選Test Classification

565
00:22:41,000 --> 00:22:42,000
那任務就選

566
00:22:42,000 --> 00:22:44,000
Sentiment Classification

567
00:22:44,000 --> 00:22:46,000
對就做情感分析的嘛

568
00:22:46,000 --> 00:22:48,000
那語言就選English

569
00:22:48,000 --> 00:22:50,000
那我最後會挑這個

570
00:22:50,000 --> 00:22:51,000
最後是挑

571
00:22:51,000 --> 00:22:52,000
YELP Review For

572
00:22:52,000 --> 00:22:53,000
這個資料集

573
00:22:53,000 --> 00:22:54,000
那這個資料的來源呢

574
00:22:54,000 --> 00:22:56,000
是YELP網站

575
00:22:56,000 --> 00:22:59,000
他們用戶對於各個店家的評論

576
00:22:59,000 --> 00:23:01,000
就類似Google評論這樣子

577
00:23:01,000 --> 00:23:03,000
對那你也不一定要跟我用一樣的

578
00:23:03,000 --> 00:23:06,000
對這邊只是用這個當作範例

579
00:23:06,000 --> 00:23:08,000
好

580
00:23:08,000 --> 00:23:10,000
那接下來要下載資料集的話

581
00:23:10,000 --> 00:23:11,000
就用Raw Dataset

582
00:23:11,000 --> 00:23:12,000
這一個Function

583
00:23:12,000 --> 00:23:13,000
然後參數呢

584
00:23:13,000 --> 00:23:15,000
就是給他你資料集的名稱

585
00:23:15,000 --> 00:23:17,000
這樣子就下載下來

586
00:23:17,000 --> 00:23:19,000
好那我們把其中一筆資料

587
00:23:19,000 --> 00:23:21,000
印出來來看

588
00:23:21,000 --> 00:23:22,000
那我們可以看到

589
00:23:22,000 --> 00:23:24,000
他有這個Test跟Label兩個欄位

590
00:23:24,000 --> 00:23:27,000
那Test的話就是他用戶的評論

591
00:23:27,000 --> 00:23:29,000
那Label則是他的評分

592
00:23:29,000 --> 00:23:31,000
那他這個0到4的數字

593
00:23:31,000 --> 00:23:33,000
分別就代表1到5顆星

594
00:23:33,000 --> 00:23:35,000
這樣子

595
00:23:35,000 --> 00:23:38,000
好那接下來又會用到剛才的Tokenizer

596
00:23:38,000 --> 00:23:41,000
那這個時候要用的模型是Birthday Case

597
00:23:41,000 --> 00:23:44,000
待會會再提到

598
00:23:44,000 --> 00:23:47,000
好那要對資料做資料域處理的話

599
00:23:47,000 --> 00:23:49,000
就很簡單

600
00:23:49,000 --> 00:23:51,000
我們就寫好一個Function

601
00:23:51,000 --> 00:23:53,000
那輸入呢會是你的文字

602
00:23:53,000 --> 00:23:56,000
輸出就是處理好的Token

603
00:23:56,000 --> 00:23:58,000
然後再Return回去就可以了

604
00:23:58,000 --> 00:24:00,000
好那我們這邊會多加上

605
00:24:00,000 --> 00:24:02,000
Padding跟Truncation這兩個參數

606
00:24:02,000 --> 00:24:07,000
那這是為了讓你模型的輸入長度是固定的

607
00:24:07,000 --> 00:24:09,000
那如果你今天句子太短

608
00:24:09,000 --> 00:24:11,000
他會在後面補上Padding

609
00:24:11,000 --> 00:24:13,000
那如果太長則是會直接Truncate

610
00:24:13,000 --> 00:24:15,000
就是直接截斷掉

611
00:24:15,000 --> 00:24:17,000
好那再來就是

612
00:24:17,000 --> 00:24:19,000
使用Dataset.map這個Function

613
00:24:19,000 --> 00:24:22,000
就可以把你這個資料集裡面的文字

614
00:24:22,000 --> 00:24:24,000
通通都轉成Token

615
00:24:24,000 --> 00:24:27,000
好那接下來資料預處理完之後

616
00:24:27,000 --> 00:24:30,000
接下來就是要來做這個Fyton

617
00:24:30,000 --> 00:24:32,000
那首先我們要先下載模型

618
00:24:32,000 --> 00:24:36,000
那我們要用的是BirdBaseCast這個模型

619
00:24:36,000 --> 00:24:38,000
那他呢就是最原始

620
00:24:38,000 --> 00:24:41,000
完全還沒有Fyton過的模型

621
00:24:41,000 --> 00:24:44,000
那我們待會就是要來Fyton這個模型

622
00:24:46,000 --> 00:24:48,000
好那我們希望我們在

623
00:24:48,000 --> 00:24:50,000
就是訓練的過程

624
00:24:50,000 --> 00:24:52,000
是有一個衡量指標的

625
00:24:52,000 --> 00:24:54,000
所以我們就要用到RoadMetric

626
00:24:54,000 --> 00:24:56,000
然後參數這邊用Occurrency呢

627
00:24:56,000 --> 00:24:58,000
就是代表我要計算的是正確率

628
00:24:58,000 --> 00:25:00,000
好那他還有提供

629
00:25:00,000 --> 00:25:02,000
其他很多的這些指標

630
00:25:02,000 --> 00:25:04,000
對那你可以再去他的Document找找看

631
00:25:04,000 --> 00:25:06,000
好那用的方法

632
00:25:06,000 --> 00:25:08,000
跟剛才的Tokenizer很像

633
00:25:08,000 --> 00:25:10,000
我們就是寫一個Function

634
00:25:10,000 --> 00:25:12,000
然後

635
00:25:12,000 --> 00:25:14,000
然後呢

636
00:25:14,000 --> 00:25:16,000
輸入就是給他Prediction跟Label

637
00:25:16,000 --> 00:25:18,000
那他就會計算這個正確率

638
00:25:18,000 --> 00:25:20,000
並且把他Return回來

639
00:25:20,000 --> 00:25:22,000
好那

640
00:25:22,000 --> 00:25:24,000
再來就是訓練過模型的人都知道

641
00:25:24,000 --> 00:25:26,000
會有所謂超參數的問題

642
00:25:26,000 --> 00:25:28,000
那在Transformance裡面

643
00:25:28,000 --> 00:25:30,000
就是用Training Arguments

644
00:25:30,000 --> 00:25:32,000
這一個物件

645
00:25:32,000 --> 00:25:34,000
那就可以用這個物件來設定

646
00:25:34,000 --> 00:25:36,000
像是APOC或去Batch Size

647
00:25:36,000 --> 00:25:38,000
這些超參數

648
00:25:38,000 --> 00:25:40,000
好那像我們這個範例裡面

649
00:25:40,000 --> 00:25:42,000
我們只設定了這個Output Directory

650
00:25:42,000 --> 00:25:44,000
這是你到時候存檔的名稱

651
00:25:44,000 --> 00:25:46,000
好然後這個

652
00:25:46,000 --> 00:25:48,000
Evaluation Strategy

653
00:25:48,000 --> 00:25:50,000
這個等於APOC

654
00:25:50,000 --> 00:25:52,000
這是代表說他每一次APOC

655
00:25:52,000 --> 00:25:54,000
他就會去呼叫上面那一個

656
00:25:54,000 --> 00:25:56,000
Compute Metric那個Function

657
00:25:56,000 --> 00:25:58,000
然後去計算一次他的正確率

658
00:25:58,000 --> 00:26:00,000
好那其餘的超參數

659
00:26:00,000 --> 00:26:02,000
我們都保持預設就可以了

660
00:26:02,000 --> 00:26:04,000
那接下來就是

661
00:26:04,000 --> 00:26:06,000
建立一個Trainer的物件

662
00:26:06,000 --> 00:26:08,000
把剛才的Models啊

663
00:26:08,000 --> 00:26:10,000
Argument然後Dataset

664
00:26:10,000 --> 00:26:12,000
還有Compute Metric這些通通都丟進去

665
00:26:12,000 --> 00:26:14,000
然後就呼叫Trainer.Train

666
00:26:14,000 --> 00:26:16,000
就結束了

667
00:26:16,000 --> 00:26:18,000
那他底下就會開始跑進度條

668
00:26:18,000 --> 00:26:20,000
那等他跑完這個模型就也Python完

669
00:26:20,000 --> 00:26:22,000
好那他底下會列出

670
00:26:22,000 --> 00:26:24,000
每一次APOC所計算出來的正確率

671
00:26:24,000 --> 00:26:26,000
那像我這邊用

672
00:26:26,000 --> 00:26:28,000
4000筆資料他大概跑了22分鐘

673
00:26:28,000 --> 00:26:30,000
那最後出來的正確率大概

674
00:26:30,000 --> 00:26:32,000
60%這樣

675
00:26:32,000 --> 00:26:34,000
好那再來講

676
00:26:34,000 --> 00:26:36,000
這個你模型Python完之後

677
00:26:36,000 --> 00:26:38,000
接下來就要把模型儲存起來

678
00:26:38,000 --> 00:26:40,000
那使用的

679
00:26:40,000 --> 00:26:42,000
就是SaveModel

680
00:26:42,000 --> 00:26:44,000
這個function

681
00:26:44,000 --> 00:26:46,000
那參數就是給他你

682
00:26:46,000 --> 00:26:48,000
指定的路徑就可以了

683
00:26:48,000 --> 00:26:50,000
那你要把他再載入回來

684
00:26:50,000 --> 00:26:52,000
也很簡單就是用fromPretrain

685
00:26:52,000 --> 00:26:54,000
那一樣給他指定的路徑

686
00:26:54,000 --> 00:26:56,000
就可以載入回來

687
00:26:56,000 --> 00:26:58,000
那這邊提一下Transformer

688
00:26:58,000 --> 00:27:00,000
有一個很強大的功能

689
00:27:00,000 --> 00:27:02,000
就是在於他可以在PyTorch

690
00:27:02,000 --> 00:27:04,000
跟TensorFlow的模型之間

691
00:27:04,000 --> 00:27:06,000
很輕易的做轉換

692
00:27:06,000 --> 00:27:08,000
那像我們剛才下載的其實是

693
00:27:08,000 --> 00:27:10,000
PyTorch的模型

694
00:27:10,000 --> 00:27:12,000
那用TensorFlow模型的話

695
00:27:12,000 --> 00:27:14,000
就是在後面加上一個

696
00:27:14,000 --> 00:27:16,000
fromPT等於true

697
00:27:16,000 --> 00:27:18,000
這樣他就會幫你把PyTorch的模型

698
00:27:18,000 --> 00:27:20,000
轉成TensorFlow的模型

699
00:27:20,000 --> 00:27:22,000
那最後

700
00:27:22,000 --> 00:27:24,000
就是來講一下如果要

701
00:27:24,000 --> 00:27:26,000
分享你PyTorch的模型的話

702
00:27:26,000 --> 00:27:28,000
要先下載這個HoggingFaceHub

703
00:27:28,000 --> 00:27:30,000
這一個套件

704
00:27:30,000 --> 00:27:32,000
接下來執行上面這段程式碼之後

705
00:27:32,000 --> 00:27:34,000
就會出現像下面這個

706
00:27:34,000 --> 00:27:36,000
提示的畫面要你登錄

707
00:27:36,000 --> 00:27:38,000
那你就點擊

708
00:27:38,000 --> 00:27:40,000
藍色這個連結

709
00:27:40,000 --> 00:27:42,000
那他會到HoggingFace的一個網站

710
00:27:42,000 --> 00:27:44,000
那你就去那邊註冊一個

711
00:27:44,000 --> 00:27:46,000
你的帳號然後新增一個

712
00:27:46,000 --> 00:27:48,000
權限是Write的Token

713
00:27:48,000 --> 00:27:50,000
再把這個Token填回到這個框框裡面

714
00:27:50,000 --> 00:27:52,000
unlogin就可以了

715
00:27:52,000 --> 00:27:54,000
好那登錄完之後

716
00:27:54,000 --> 00:27:56,000
就可以呼叫

717
00:27:56,000 --> 00:27:58,000
這個PushToHub

718
00:27:58,000 --> 00:28:00,000
把你這個PyTorch的模型

719
00:28:00,000 --> 00:28:02,000
然後TensorFlow

720
00:28:02,000 --> 00:28:04,000
跟這個Tokenizer這些通通都

721
00:28:04,000 --> 00:28:06,000
Push到HoggingFace的網站上面

722
00:28:06,000 --> 00:28:08,000
那這些參數呢

723
00:28:08,000 --> 00:28:10,000
就都是你Model的名稱

724
00:28:12,000 --> 00:28:14,000
那以後呢你就可以從

725
00:28:14,000 --> 00:28:16,000
HoggingFace的網站下載你的模型

726
00:28:16,000 --> 00:28:18,000
來使用,那這個時候的模型

727
00:28:18,000 --> 00:28:20,000
名稱就會是

728
00:28:20,000 --> 00:28:22,000
你的UserName然後一條斜線

729
00:28:22,000 --> 00:28:24,000
再加上你的ModelName,那這樣子

730
00:28:24,000 --> 00:28:26,000
他就會把模型下載下來

731
00:28:26,000 --> 00:28:28,000
那像這邊我就把剛才PyTorch好的模型

732
00:28:28,000 --> 00:28:30,000
給下載下來,分別就叫

733
00:28:30,000 --> 00:28:32,000
MyModel跟MyTokenizer

734
00:28:33,000 --> 00:28:36,000
好,那接下來就可以用

735
00:28:36,000 --> 00:28:38,000
我們剛才PyTorch好的模型

736
00:28:38,000 --> 00:28:40,000
來做這個Sentiment Analysis

737
00:28:42,000 --> 00:28:44,000
對,那結果呢就還不錯

738
00:28:44,000 --> 00:28:46,000
那最後提一下

739
00:28:46,000 --> 00:28:48,000
這個網站的部分

740
00:28:48,000 --> 00:28:50,000
那你可以到HoggingFace的網站

741
00:28:50,000 --> 00:28:52,000
來看自己的模型

742
00:28:52,000 --> 00:28:54,000
那像是這個是以我的模型

743
00:28:54,000 --> 00:28:56,000
來當作例子,那這邊有一個

744
00:28:56,000 --> 00:28:58,000
Edit Model Card

745
00:28:58,000 --> 00:29:00,000
這就類似你在GitHub上面寫ReadMe一樣

746
00:29:00,000 --> 00:29:03,000
然後這個Host Inference API呢

747
00:29:03,000 --> 00:29:06,000
則是可以讓你簡單的對你的

748
00:29:06,000 --> 00:29:08,000
模型做一些測試

749
00:29:10,000 --> 00:29:12,000
好,那最後因為PyTorch這個流程

750
00:29:12,000 --> 00:29:14,000
就是比較多,所以我講得比較快

751
00:29:14,000 --> 00:29:16,000
那最後幫大家統整一下

752
00:29:16,000 --> 00:29:18,000
整個流程

753
00:29:18,000 --> 00:29:20,000
那首先,就是你要先決定

754
00:29:20,000 --> 00:29:22,000
你要做的任務是哪一個類型

755
00:29:22,000 --> 00:29:24,000
好,那再來呢

756
00:29:24,000 --> 00:29:26,000
就是要針對你

757
00:29:26,000 --> 00:29:28,000
這個任務來去找

758
00:29:28,000 --> 00:29:30,000
適合的資料集

759
00:29:30,000 --> 00:29:32,000
然後以及對這個資料做預處理

760
00:29:32,000 --> 00:29:34,000
好,那再來呢,就是要找到

761
00:29:34,000 --> 00:29:36,000
適合的模型

762
00:29:36,000 --> 00:29:38,000
然後來對它做Fyton

763
00:29:38,000 --> 00:29:40,000
那Fyton完之後呢

764
00:29:40,000 --> 00:29:42,000
就可以把模型儲存起來

765
00:29:42,000 --> 00:29:44,000
或者是看你要不要分享出去

766
00:29:44,000 --> 00:29:46,000
好,那我們實作的部分就講到這邊

767
00:29:48,000 --> 00:29:50,000
好,那最後是一些Reference

768
00:29:50,000 --> 00:29:52,000
那這些就是

769
00:29:52,000 --> 00:29:54,000
這個投影片的一些參考資料

770
00:29:54,000 --> 00:29:56,000
那其中很推薦大家去看這個

771
00:29:56,000 --> 00:29:58,000
Reference的Document

772
00:29:58,000 --> 00:30:00,000
它寫得很豐富也很完整

773
00:30:00,000 --> 00:30:02,000
那這個實作部分也有部分是參考

774
00:30:02,000 --> 00:30:04,000
這個Document

775
00:30:04,000 --> 00:30:06,000
所以如果對NLP有興趣的人呢

776
00:30:06,000 --> 00:30:08,000
很推薦去看

777
00:30:08,000 --> 00:30:10,000
好,那最後是

778
00:30:10,000 --> 00:30:12,000
做個小總結

779
00:30:12,000 --> 00:30:14,000
好,那今天呢

780
00:30:14,000 --> 00:30:16,000
有介紹這個NLP的

781
00:30:16,000 --> 00:30:18,000
簡介、實際應用、發展史

782
00:30:18,000 --> 00:30:20,000
還有實作

783
00:30:20,000 --> 00:30:22,000
那基本上這邊就只是

784
00:30:22,000 --> 00:30:24,000
NLP的冰山一角而已

785
00:30:24,000 --> 00:30:26,000
很多很多都沒有時間講

786
00:30:26,000 --> 00:30:28,000
所以呢

787
00:30:28,000 --> 00:30:30,000
歡迎如果對NLP有興趣的人

788
00:30:30,000 --> 00:30:32,000
可以回家自行研究

789
00:30:32,000 --> 00:30:34,000
或者是待會會後來找我一起討論

790
00:30:34,000 --> 00:30:36,000
讓大家共創一個良好的社群環境

791
00:30:38,000 --> 00:30:40,000
好,那最後呢

792
00:30:40,000 --> 00:30:42,000
這個議程的投影片跟範例程式碼

793
00:30:42,000 --> 00:30:44,000
都有公開在GitHub上面

794
00:30:44,000 --> 00:30:46,000
大家可以自由地使用

795
00:30:46,000 --> 00:30:48,000
那右邊這個是我的個人網站

796
00:30:48,000 --> 00:30:50,000
有什麼問題也歡迎

797
00:30:50,000 --> 00:30:52,000
透過這個會後來聯絡我

798
00:30:52,000 --> 00:30:54,000
那今天的分享就到這邊

799
00:30:54,000 --> 00:30:56,000
謝謝大家

800
00:31:02,000 --> 00:31:04,000
好,那再來就

801
00:31:04,000 --> 00:31:06,000
直接進Q&A環節

802
00:31:22,000 --> 00:31:26,000
希望有正常一點的問題

803
00:31:26,000 --> 00:31:28,000
好,那

804
00:31:28,000 --> 00:31:30,000
首先

805
00:31:30,000 --> 00:31:32,000
為什麼這個站那麼多

806
00:31:32,000 --> 00:31:36,000
NLP可以了解你的梗圖嗎

807
00:31:36,000 --> 00:31:38,000
我覺得

808
00:31:38,000 --> 00:31:40,000
你是指什麼

809
00:31:40,000 --> 00:31:42,000
如果是要分析梗圖的話

810
00:31:42,000 --> 00:31:44,000
我覺得那應該是影像辨識的東西

811
00:31:44,000 --> 00:31:46,000
我覺得應該不是NLP的

812
00:31:46,000 --> 00:31:48,000
對

813
00:31:48,000 --> 00:31:50,000
好,然後

814
00:31:50,000 --> 00:31:52,000
然後

815
00:31:52,000 --> 00:31:54,000
可以再給一次PTT連結嗎

816
00:31:54,000 --> 00:31:56,000
那

817
00:31:56,000 --> 00:31:58,000
這個PTT連結的話

818
00:31:58,000 --> 00:32:00,000
你到Seacount的議程表

819
00:32:00,000 --> 00:32:02,000
然後我的議程詳細資訊那邊

820
00:32:02,000 --> 00:32:04,000
就有提供

821
00:32:04,000 --> 00:32:06,000
就投影片的連結

822
00:32:06,000 --> 00:32:08,000
大家可以去那邊

823
00:32:08,000 --> 00:32:10,000
看你要下載還是怎樣都可以

824
00:32:10,000 --> 00:32:12,000
好,然後

825
00:32:12,000 --> 00:32:14,000
這部電影真有趣的

826
00:32:14,000 --> 00:32:16,000
score很低只有0.7多

827
00:32:16,000 --> 00:32:18,000
呃

828
00:32:18,000 --> 00:32:20,000
這個就不太確定

829
00:32:20,000 --> 00:32:22,000
因為它是模型

830
00:32:22,000 --> 00:32:24,000
內部決定的

831
00:32:24,000 --> 00:32:26,000
所以其實

832
00:32:26,000 --> 00:32:28,000
為什麼分數會這樣

833
00:32:28,000 --> 00:32:30,000
我也很難跟你講為什麼

834
00:32:30,000 --> 00:32:32,000
好

835
00:32:32,000 --> 00:32:34,000
那這個問題大概就

836
00:32:34,000 --> 00:32:36,000
這樣子

837
00:32:36,000 --> 00:32:38,000
那個分數其實是它

838
00:32:38,000 --> 00:32:40,000
模型對這個

839
00:32:40,000 --> 00:32:42,000
對這個句子的信心分數

840
00:32:42,000 --> 00:32:44,000
所以也

841
00:32:44,000 --> 00:32:46,000
其實應該也不能代表說

842
00:32:46,000 --> 00:32:48,000
正向程度有比較高

843
00:32:50,000 --> 00:32:52,000
那下一個問題

844
00:32:52,000 --> 00:32:54,000
Bert一直

845
00:32:54,000 --> 00:32:56,000
以來都有壓縮收斂

846
00:32:56,000 --> 00:32:58,000
速度過快的問題

847
00:32:58,000 --> 00:33:00,000
這會導致模型針對

848
00:33:00,000 --> 00:33:02,000
句子中的特定詞彙比重

849
00:33:02,000 --> 00:33:04,000
被放大 進而導致

850
00:33:04,000 --> 00:33:06,000
錯誤的結果

851
00:33:06,000 --> 00:33:08,000
針對這部分有什麼特別的解法呢

852
00:33:08,000 --> 00:33:18,000
這個問題喔

853
00:33:18,000 --> 00:33:20,000
這個問題的話

854
00:33:20,000 --> 00:33:22,000
其實後續就有一些

855
00:33:22,000 --> 00:33:24,000
模型或一些論文

856
00:33:24,000 --> 00:33:26,000
就是在針對這個問題

857
00:33:26,000 --> 00:33:28,000
來做解決

858
00:33:28,000 --> 00:33:30,000
對 所以

859
00:33:30,000 --> 00:33:32,000
其實大家可以去看

860
00:33:32,000 --> 00:33:34,000
後續一些比較新的模型

861
00:33:34,000 --> 00:33:36,000
因為像是Bert也是2018年

862
00:33:36,000 --> 00:33:38,000
那其實離現在也大概四年了

863
00:33:38,000 --> 00:33:40,000
其實後續就一直都有推出

864
00:33:40,000 --> 00:33:42,000
新的模型

865
00:33:42,000 --> 00:33:44,000
那也有試圖解決Bert的這個問題

866
00:33:44,000 --> 00:33:46,000
那大家可以再去

867
00:33:46,000 --> 00:33:48,000
追蹤一下新的paper

868
00:33:48,000 --> 00:33:50,000
有沒有關於這部分

869
00:33:50,000 --> 00:33:52,000
的這個解法這樣子

870
00:33:52,000 --> 00:33:54,000
好

871
00:33:54,000 --> 00:33:56,000
那

872
00:33:56,000 --> 00:33:58,000
那那個

873
00:33:58,000 --> 00:34:00,000
NLP裡面會如何解決

874
00:34:00,000 --> 00:34:02,000
overfitting的問題喔

875
00:34:02,000 --> 00:34:04,000
嗯

876
00:34:04,000 --> 00:34:06,000
嗯

877
00:34:10,000 --> 00:34:12,000
overfitting

878
00:34:12,000 --> 00:34:14,000
那基本上

879
00:34:14,000 --> 00:34:16,000
這個做法跟你

880
00:34:16,000 --> 00:34:18,000
一般的那種

881
00:34:18,000 --> 00:34:20,000
機器學習的做法

882
00:34:20,000 --> 00:34:22,000
其實都差不多

883
00:34:22,000 --> 00:34:24,000
就是你可能要去算它的那個

884
00:34:24,000 --> 00:34:26,000
validation

885
00:34:26,000 --> 00:34:28,000
的那個

886
00:34:28,000 --> 00:34:30,000
occurrency

887
00:34:30,000 --> 00:34:32,000
就是算那個validation

888
00:34:32,000 --> 00:34:34,000
看有沒有發生overfitting的問題

889
00:34:34,000 --> 00:34:36,000
這樣子

890
00:34:36,000 --> 00:34:38,000
那至於如何解決的話

891
00:34:38,000 --> 00:34:40,000
我現在沒有一個很直接的

892
00:34:42,000 --> 00:34:44,000
沒有一個很直接的答案要怎麼解決

893
00:34:44,000 --> 00:34:46,000
好那

894
00:34:46,000 --> 00:34:48,000
下一個問題是

895
00:34:48,000 --> 00:34:50,000
中文斷詞的心得

896
00:34:50,000 --> 00:34:52,000
那中文斷詞的話

897
00:34:52,000 --> 00:34:54,000
Python有一些套件可以使用

898
00:34:54,000 --> 00:34:56,000
像是那個J8

899
00:34:56,000 --> 00:34:58,000
這個應該滿有名的

900
00:34:58,000 --> 00:35:00,000
然後還有

901
00:35:00,000 --> 00:35:02,000
我記得有一個

902
00:35:02,000 --> 00:35:04,000
我忘記是中研院還是什麼

903
00:35:04,000 --> 00:35:06,000
有做出一個斷詞的系統

904
00:35:06,000 --> 00:35:08,000
好像叫

905
00:35:08,000 --> 00:35:10,000
C什麼開頭有點忘記了

906
00:35:10,000 --> 00:35:12,000
那個

907
00:35:12,000 --> 00:35:14,000
它是做繁中的斷詞

908
00:35:14,000 --> 00:35:16,000
它的斷詞的效果

909
00:35:16,000 --> 00:35:18,000
也還不錯

910
00:35:18,000 --> 00:35:20,000
大家可以去用用看

911
00:35:20,000 --> 00:35:22,000
然後

912
00:35:22,000 --> 00:35:24,000
有沒有試

913
00:35:24,000 --> 00:35:26,000
有沒有試過文言文

914
00:35:26,000 --> 00:35:28,000
我自己是沒有試過

915
00:35:28,000 --> 00:35:30,000
但是好像有看過

916
00:35:30,000 --> 00:35:32,000
有些

917
00:35:32,000 --> 00:35:34,000
就是論文或是

918
00:35:34,000 --> 00:35:36,000
之前有一些

919
00:35:36,000 --> 00:35:38,000
project好像有在做這方面的

920
00:35:38,000 --> 00:35:40,000
對

921
00:35:40,000 --> 00:35:42,000
我自己是沒有試過

922
00:35:42,000 --> 00:35:44,000
但可以去嘗試看看

923
00:35:44,000 --> 00:35:46,000
然後

924
00:35:46,000 --> 00:35:48,000
NLP能理解

925
00:35:48,000 --> 00:35:50,000
好棒跟啊不就好棒棒

926
00:35:50,000 --> 00:35:52,000
的情感差異嗎

927
00:35:52,000 --> 00:35:54,000
我覺得你可以試試看

928
00:35:54,000 --> 00:35:56,000
我覺得可以試試看

929
00:35:56,000 --> 00:35:58,000
看他分數有沒有差

930
00:35:58,000 --> 00:36:00,000
我是沒試過

931
00:36:00,000 --> 00:36:02,000
好那請問

932
00:36:02,000 --> 00:36:04,000
GPU用爆是什麼情況

933
00:36:04,000 --> 00:36:06,000
我就是一直勸啊

934
00:36:06,000 --> 00:36:08,000
他一跑完就

935
00:36:08,000 --> 00:36:10,000
他跑完就繼續給他勸下去

936
00:36:10,000 --> 00:36:12,000
就是你就一邊開著

937
00:36:12,000 --> 00:36:14,000
然後一邊打LOL

938
00:36:14,000 --> 00:36:16,000
打完那個模型也差不多就跑完了

939
00:36:16,000 --> 00:36:18,000
而且他也不是吃你的

940
00:36:18,000 --> 00:36:20,000
吃你電腦的效能

941
00:36:20,000 --> 00:36:22,000
所以根本就沒差

942
00:36:22,000 --> 00:36:24,000
然後有時候就是

943
00:36:24,000 --> 00:36:26,000
你打LOL完回來看

944
00:36:26,000 --> 00:36:28,000
然後你就會看到那個

945
00:36:28,000 --> 00:36:30,000
就是說你的GPU已經用爆了

946
00:36:30,000 --> 00:36:32,000
然後他就直接幫你停掉

947
00:36:32,000 --> 00:36:34,000
然後你之後就會被BAN一段時間

948
00:36:34,000 --> 00:36:36,000
大概

949
00:36:36,000 --> 00:36:38,000
看情況

950
00:36:38,000 --> 00:36:40,000
有時候可能會BAN個半天一天

951
00:36:40,000 --> 00:36:42,000
那有時候也蠻快的

952
00:36:42,000 --> 00:36:44,000
幾小時之後就可以用

953
00:36:44,000 --> 00:36:46,000
好然後

954
00:36:48,000 --> 00:36:50,000
在資料分析的前提下

955
00:36:50,000 --> 00:36:52,000
自行的開發

956
00:36:52,000 --> 00:36:54,000
開發也越來越多樣

957
00:36:54,000 --> 00:36:56,000
什麼Python

958
00:36:56,000 --> 00:36:58,000
Unicode data normalized

959
00:36:58,000 --> 00:37:00,000
可以使用

960
00:37:00,000 --> 00:37:02,000
保證正規化後不會跑板

961
00:37:02,000 --> 00:37:04,000
並供續後續研究

962
00:37:04,000 --> 00:37:06,000
例如將

963
00:37:06,000 --> 00:37:08,000
字體正規化後

964
00:37:08,000 --> 00:37:10,000
針對其國籍區別並做單詞分析

965
00:37:14,000 --> 00:37:16,000
這個問題是什麼

966
00:37:18,000 --> 00:37:20,000
有點看不懂這個問題

967
00:37:20,000 --> 00:37:24,000
如果有在場的話可以解釋一下

968
00:37:26,000 --> 00:37:28,000
我們先跳到下一個問題

969
00:37:28,000 --> 00:37:32,000
NLP有機會設計出解釋程式碼的模型嗎

970
00:37:32,000 --> 00:37:34,000
其實現在就有啦

971
00:37:34,000 --> 00:37:36,000
GitHub有一個

972
00:37:36,000 --> 00:37:38,000
叫Code

973
00:37:38,000 --> 00:37:40,000
反正GitHub有

974
00:37:40,000 --> 00:37:42,000
自己有做一個模型

975
00:37:42,000 --> 00:37:44,000
叫什麼Copilot

976
00:37:44,000 --> 00:37:46,000
Pilot之類的

977
00:37:46,000 --> 00:37:48,000
那一個

978
00:37:48,000 --> 00:37:51,000
它就可以自動幫你生成

979
00:37:51,000 --> 00:37:53,000
程式語言

980
00:37:55,000 --> 00:37:58,000
它就可以幫你生成程式語言

981
00:37:58,000 --> 00:38:00,000
你可能只要寫個註解

982
00:38:00,000 --> 00:38:02,000
它就可以幫你

983
00:38:02,000 --> 00:38:04,000
Code Pilot

984
00:38:06,000 --> 00:38:09,000
它就會幫你生成程式語言

985
00:38:09,000 --> 00:38:10,000
你甚至只要寫註解

986
00:38:10,000 --> 00:38:11,000
它就可以幫你

987
00:38:11,000 --> 00:38:13,000
把對應的程式碼都生出來

988
00:38:13,000 --> 00:38:16,000
這其實也就是NLP的應用之一

989
00:38:16,000 --> 00:38:20,000
像是GitHub就是拿他們網站上面

990
00:38:20,000 --> 00:38:24,000
就是大家放在他們網站上面的那些程式碼

991
00:38:24,000 --> 00:38:26,000
來訓練的模型

992
00:38:26,000 --> 00:38:28,000
但是現在要收費

993
00:38:28,000 --> 00:38:29,000
很討厭

994
00:38:31,000 --> 00:38:36,000
這個NLP分得出哈哈哈哈哈哈的差別嗎

995
00:38:39,000 --> 00:38:42,000
我覺得可以試試看

996
00:38:42,000 --> 00:38:44,000
但其實它斷詞應該會

997
00:38:44,000 --> 00:38:46,000
就斷成一個一個字

998
00:38:48,000 --> 00:38:50,000
就是可以試試看

999
00:38:50,000 --> 00:38:52,000
大家都很天罵星空

1000
00:38:53,000 --> 00:38:56,000
打到被嘴砲可以用NLP分析嗎

1001
00:38:56,000 --> 00:38:58,000
可以啊你可以訓練一個那個

1002
00:38:58,000 --> 00:39:00,000
那個嘴砲機器人

1003
00:39:00,000 --> 00:39:01,000
就有人嘴你

1004
00:39:01,000 --> 00:39:04,000
然後你就可以用機器人嘴回去了

1005
00:39:04,000 --> 00:39:06,000
可以試試看

1006
00:39:06,000 --> 00:39:08,000
做這樣的模型

1007
00:39:11,000 --> 00:39:13,000
我用過最大的模型

1008
00:39:13,000 --> 00:39:15,000
我想一下喔

1009
00:39:18,000 --> 00:39:20,000
最大的模型

1010
00:39:22,000 --> 00:39:24,000
我之前有用過

1011
00:39:24,000 --> 00:39:26,000
最大的

1012
00:39:28,000 --> 00:39:30,000
我之前有用過T5模型

1013
00:39:30,000 --> 00:39:32,000
那它應該是

1014
00:39:32,000 --> 00:39:34,000
我目前用過最大的模型

1015
00:39:34,000 --> 00:39:36,000
這樣子

1016
00:39:36,000 --> 00:39:38,000
好那

1017
00:39:38,000 --> 00:39:40,000
對那剛才我講的就是這個

1018
00:39:40,000 --> 00:39:42,000
GitHub的Copied

1019
00:39:42,000 --> 00:39:44,000
那它就是做程式碼

1020
00:39:44,000 --> 00:39:46,000
的這個自然語言的分析

1021
00:39:46,000 --> 00:39:48,000
對

1022
00:39:48,000 --> 00:39:50,000
好那Test

1023
00:39:50,000 --> 00:39:52,000
出來分數很低的話

1024
00:39:52,000 --> 00:39:54,000
有什麼方法可以改善模型嗎

1025
00:39:56,000 --> 00:39:58,000
呃Test出來的話

1026
00:40:00,000 --> 00:40:02,000
我覺得

1027
00:40:02,000 --> 00:40:04,000
狀況很多耶因為Test出來分數低其實

1028
00:40:04,000 --> 00:40:06,000
有蠻多因素的

1029
00:40:06,000 --> 00:40:08,000
你可能要針對你的問題來去做改善

1030
00:40:08,000 --> 00:40:10,000
那

1031
00:40:10,000 --> 00:40:12,000
請問Tokenizer

1032
00:40:12,000 --> 00:40:14,000
也是一種模型嗎

1033
00:40:14,000 --> 00:40:16,000
喔不是喔Tokenizer它不是一種模型

1034
00:40:16,000 --> 00:40:18,000
它其實是一個

1035
00:40:18,000 --> 00:40:20,000
有點類似工具它可以幫你

1036
00:40:20,000 --> 00:40:22,000
轉換那些字

1037
00:40:22,000 --> 00:40:24,000
這樣子

1038
00:40:24,000 --> 00:40:26,000
那

1039
00:40:26,000 --> 00:40:28,000
然後

1040
00:40:28,000 --> 00:40:30,000
像是因為它不同的Model

1041
00:40:30,000 --> 00:40:32,000
它會有一些比較特別的Token

1042
00:40:32,000 --> 00:40:34,000
那不同Model

1043
00:40:34,000 --> 00:40:36,000
它那些特別的Token會不一樣

1044
00:40:36,000 --> 00:40:38,000
所以

1045
00:40:38,000 --> 00:40:40,000
不同的Tokenizer就會

1046
00:40:40,000 --> 00:40:42,000
就會幫你做不一樣的轉換

1047
00:40:42,000 --> 00:40:44,000
這樣子

1048
00:40:54,000 --> 00:40:56,000
那因為時間到了

1049
00:40:56,000 --> 00:40:58,000
那我今天的報告就到這邊

1050
00:40:58,000 --> 00:41:00,000
那會後有什麼問題的話也

1051
00:41:00,000 --> 00:41:02,000
歡迎再來找我討論

1052
00:41:02,000 --> 00:41:04,000
好謝謝大家

